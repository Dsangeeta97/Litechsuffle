# -*- coding: utf-8 -*-
"""Untitled22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V6IJeU3lWhZnzFtcDGmUEhpeenl3Lzh_
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision.models import shufflenet_v2_x1_0
from torchsummary import summary


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


teacher_model = shufflenet_v2_x1_0(pretrained=True)
teacher_model.fc = nn.Linear(in_features=1024, out_features=39)  # 39 classes in dataset
teacher_model = torch.load('teacher_model.pth', map_location=torch.device('cuda'))  #load trained teacher model
teacher_model.to(device)
teacher_model.eval()

summary(teacher_model, input_size=(3, 224, 224))

student_model =  torch.load('student_model.pth', map_location=torch.device('cuda'))  # load trained student model

student_model.to(device)
student_model.train()




class KnowledgeDistillationLoss(nn.Module):
    def __init__(self, temperature=1.0, alpha=0.3):
        super(KnowledgeDistillationLoss, self).__init__()
        self.temperature = temperature
        self.alpha = alpha

    def forward(self, student_outputs, teacher_outputs, labels):
        # Soft targets (teacher)
        soft_targets = F.softmax(teacher_outputs / self.temperature, dim=1)
        soft_loss = -(soft_targets * F.log_softmax(student_outputs / self.temperature, dim=1)).sum(dim=1).mean()

        # Hard loss (student vs ground truth)
        hard_loss = F.cross_entropy(student_outputs, labels)

        # Weighted sum
        loss = self.alpha * soft_loss + (1.0 - self.alpha) * hard_loss
        return loss

# Loss function
kd_loss_fn = KnowledgeDistillationLoss(alpha=0.3, temperature=1.0)


optimizer = optim.Adam(student_model.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

num_epochs = 30
for epoch in range(num_epochs):
    student_model.train()
    total_loss = 0.0

    for inputs, labels in train_dataloader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        # Teacher outputs (no grad)
        with torch.no_grad():
            teacher_outputs = teacher_model(inputs)

        # Student outputs
        student_outputs = student_model(inputs)

        # Compute KD loss
        loss = kd_loss_fn(student_outputs, teacher_outputs, labels)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    scheduler.step()  # Learning rate update

    print(f"Epoch [{epoch+1}/{num_epochs}] - Loss: {total_loss / len(train_dataloader):.4f}")

torch.save(student_model.state_dict(), "student_model_kd.pth")